<?xml version="1.0" encoding="UTF-8" ?><ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3"><Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0"><bitmap>media/images/box/root.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram><Box name="CAIRclient multiparty" id="1" localization="8" tooltip="This box contains a basic python script and can be used to create any python script box you would like.&#x0A;&#x0A;To edit its script, double-click on it." x="387" y="142"><bitmap>media/images/box/box-python-script.png</bitmap><script language="4"><content><![CDATA[#!usr/bin/python -tt
# -*- coding: utf-8 -*-
import qi
from naoqi import ALProxy
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
app_name = "cairclient_multiparty"
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/libs/")
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/")
from cairlib.DialogueStatistics import DialogueStatistics
from cairlib.DialogueTurn import DialogueTurn
from cairlib.DialogueState import DialogueState
from cairlib.CAIRclient_SoftBank_utils import Utils
from cairlib.CAIRclient_SoftBank_actions import ActionManager
import xml.etree.ElementTree as ET
import re
import xml
import requests
import os
import json
import socket
import zlib
import threading
import random
import base64
import time
from datetime import datetime, timedelta

# Location of the server
cineca = "131.175.205.146"
raspberry = "192.168.1.101"
local_pc = "130.251.13.111"
lab_server = "130.251.13.192"
server_ip = local_pc
audio_recorder_ip = local_pc
registration_ip = local_pc
log_service_ip = local_pc

openai = True
if openai:
    server_port = "12345"
else:
    server_port = "5000"
request_uri = "http://" + server_ip + ":" + server_port + "/CAIR_hub"

# Dense captioning - if set to false the dense_cap_result will be sent empty to the server and the
# visual information will not be used by gpt-4
dense_cap = True
dense_cap_result = []
img_port = "12348"
img_url = "http://" + server_ip + ":" + img_port + "/CAIR_dense_captioning"
max_history_turns = 6
language = "it"


class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        # Instances of the classes of the other files in the libs folder containing functions needed here
        self.memory = ALProxy("ALMemory")
        self.speech_reco = ALProxy("ALSpeechRecognition")
        self.speech_reco.setAudioExpression(False)
        self.logger.info("Setting voice speed")
        try:
            self.voice_speed = "\\RSPD=100\\"
            self.memory.insertData("CAIR/voice_speed", 100)
            # self.voice_speed = "\\RSPD=" + str(self.memory.getData("CAIR/voice_speed")) + "\\"
        except:
            self.memory.insertData("CAIR/voice_speed", 100)
            self.voice_speed = "\\RSPD=100\\"

        self.memory.insertData("CAIR/server_ip", server_ip)
        self.memory.insertData("CAIR/server_port", server_port)
        self.memory.insertData("CAIR/registration_ip", registration_ip)
        self.memory.insertData("CAIR/language", language)
        self.memory.insertData("CAIR/app_name", app_name)
        self.plans = ActionManager(self.logger.info)
        self.utils = Utils(self.logger.info)
        self.isAlive = True
        self.exit_keywords = ["stop talking", "esci dallapp", "esci dallapplicazione", "quit the application"]
        self.repeat_keywords = ["repeat", "can you repeat", "say it again", "puoi ripetere", "ripeti", "non ho capito"]
        self.speech_reco_event = "Audio/RecognizedWords"
        self.al = ALProxy("ALAutonomousLife")
        self.motion = ALProxy("ALMotion")
        self.tts = ALProxy("ALTextToSpeech")
        if language == "it":
            self.tts.setLanguage("Italian")
            self.not_installed_behavior = "Mi dispiace, non sono ancora capace di svolgere questa azione perché l'applicazione non è installata."
        else:
            self.tts.setLanguage("English")
            self.not_installed_behavior = "I'm sorry, I am not able to perform this task as the application is not installed."
        self.animated_speech = ALProxy("ALAnimatedSpeech")
        self.configuration = {"bodyLanguageMode": "contextual"}
        self.behavior_manager = ALProxy("ALBehaviorManager")
        self.audio_device = ALProxy("ALAudioDevice")
        self.dialogue_state_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                        "dialogue_state.json"
        self.speakers_info_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                       "speakers_info.json"
        self.dialogue_statistics_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                             "dialogue_statistics.json"
        self.response_times_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                        "response_times.txt"

        self.nuance_vectors_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                        "nuance_vectors.json"

        self.logger.info("Trying to connect to ALTabletService...")
        self.tablet = True
        try:
            self.tablet_service = ALProxy("ALTabletService")
        except:
            self.tablet = False

        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.log_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        self.audio_device = ALProxy("ALAudioDevice")
        self.audio_device.setOutputVolume(50)

        # To store the previous sentence said by the robot
        self.previous_sentence = ""
        self.dialogue_sentence = []
        self.dialogue_state = {}
        self.dialogue_statistics = {}
        self.speakers_info = {}
        self.nuance_vectors = {}
        self.conversation_history = {}
        self.plan_sentence = ""
        self.plan = ""
        # This variable tells if the user want the robot to repeat a sentence
        self.repeat = False

        # Read the sentences in the correct language from the file
        if language == "it":
            sentences_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/sentences_it.txt"
        else:
            sentences_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/sentences_en.txt"
        self.sentences = []
        with open(sentences_file_path) as f:
            self.sentences = [line.rstrip() for line in f]

        self.dense_cap = dense_cap
        self.dense_cap_result = dense_cap_result
        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        self.photoCaptureProxy = ALProxy("ALPhotoCapture", "127.0.0.1", 9559)
        self.photoCaptureProxy.setResolution(2)
        self.photoCaptureProxy.setPictureFormat("jpg")

    def check_socket_connection(self, received_string):
        if received_string == "":
            if language == "it":
                to_say = "Mi dispiace, c'è stato qualche problema con la connessione al microfono esterno."
            else:
                to_say = "I'm sorry, there was a problem with the connection to the external microphone."
            self.animated_speech.say(self.voice_speed + to_say, self.configuration)
            self.onInput_onStop()

    def acquire_image(self):
        while True:
            now = datetime.now() + timedelta(hours=1)
            date_time_str = now.strftime("%Y-%m-%d %H:%M:%S")
            # Capture an image an take the time
            image_capture_start_time = time.time()
            self.photoCaptureProxy.takePictures(1, "/home/nao/recordings/cameras/", "camera_image", True)
            with open("/home/nao/recordings/cameras/camera_image.jpg", "rb") as img_file:
                img_encoded = base64.b64encode(img_file.read()).decode('utf-8')
            image_capture_end_time = time.time()
            image_byte_size = (len(img_encoded)*3)/4

            # Create a dictionary with the encoded image
            data = {"frame": img_encoded}
            # Send the image to the server using a POST request
            try:
                # Log the time needed to perform the request and get the response
                request_start_time = time.time()
                response = requests.post(img_url, json=data)
                self.dense_cap_result = response.json()["result"]
                request_end_time = time.time()
                to_log = "v#timestamp:" + date_time_str + "\n"
                self.log_socket.send(to_log.encode('utf-8'))
                to_log = "v#image_capture_time:" + str(image_capture_end_time - image_capture_start_time) + "\n"
                self.log_socket.send(to_log.encode('utf-8'))
                to_log = "v#compressed_image_size_bytes:" + str(image_byte_size) + "\n"
                self.log_socket.send(to_log.encode('utf-8'))
                to_log = "v#densecap_request_response_time:" + str(request_end_time - request_start_time) + "\n"
                self.log_socket.send(to_log.encode('utf-8'))
                self.log_socket.send("v#********************\n".encode('utf-8'))
            except ConnectionError:
                self.logger.info("** The dense captioning service is not available")
                exit(1)

    def say_sentence(self):
        # When using openAI say something to fill the void while waiting
        if openai:
            now = datetime.now() + timedelta(hours=1)
            date_time_str = now.strftime("%Y-%m-%d %H:%M:%S")
            to_log = "d#timestamp:" + date_time_str + "\n"
            self.log_socket.send(to_log.encode('utf-8'))
            to_say = str(random.choice(self.sentences))
            # Log the time taken to say the random sentence
            start_time = time.time()
            self.animated_speech.say(self.voice_speed + to_say, self.configuration)
            end_time = time.time()
            to_log = "d#ack_sentence_speaking_time:" + str(end_time-start_time) + "\n"
            self.log_socket.send(to_log.encode('utf-8'))

    def hub_request(self, data):
        encoded_data = json.dumps(data).encode('utf-8')
        compressed_data = zlib.compress(encoded_data)
        start_time = time.time()
        hub_response = requests.put(request_uri, data=compressed_data, verify=False)
        end_time = time.time()
        # If the Hub cannot contact the dialogue service, the response will be empty
        if hub_response:
            # Overwrite the array containing the states of the profiles with those contained in the Hub response
            # The speakers info are not sent to the Hub.
            self.dialogue_state = DialogueState(d=hub_response.json()['dialogue_state'])
            # Store the updated dialogue state in the file
            with open(self.dialogue_state_file_path, 'w') as f:
                json.dump(self.dialogue_state.to_dict(), f, ensure_ascii=False, indent=4)
            self.dialogue_sentence = hub_response.json()['dialogue_sentence']

            if data["req_type"] == 1:
                to_log = "d#first_request_response_time:" + str(end_time - start_time) + "\n"
                self.dialogue_statistics = DialogueStatistics(d=hub_response.json()["dialogue_statistics"])
                # The hub updates the average topic distance matrix, hence it should be written on the file
                with open(self.dialogue_statistics_file_path, 'w') as f:
                    json.dump(self.dialogue_statistics.to_dict(), f, ensure_ascii=False, indent=4)
                self.plan_sentence = hub_response.json()['plan_sentence']
                self.plan = hub_response.json()['plan']
            else:
                to_log = "d#second_request_response_time:" + str(end_time - start_time) + "\n"
            self.log_socket.send(to_log.encode('utf-8'))
        else:
            print("No response received from the Hub!")
            exit(0)

    def onInput_onStart(self):
        # Try connecting to the socket that records the audio
        self.logger.info("Trying to connect to the sockets.")
        try:
            self.client_socket.connect((audio_recorder_ip, 9090))
            self.log_socket.connect((log_service_ip, 9092))
        except:
            if language == "it":
                to_say = "Mi dispiace, non riesco a connettermi al microfono esterno o al servizio di log."
            else:
                to_say = "I'm sorry, I can't connect to external microphone."
            self.animated_speech.say(self.voice_speed + to_say, self.configuration)
            self.onInput_onStop()
        try:
            if self.al.getState() != "interactive":
                self.al.setState("interactive")
        except:
            pass

        # Wake the robot up if not already up
        self.motion.wakeUp()

        # With Pepper robot, preload the images to be set on the tablet during the conversation
        if self.tablet:
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/RICE.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/DialogueMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/ExecutionMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/PrivacyMode.png")
            self.tablet_service.showImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/RICE.png")

        # If it's the first time using the system, call the function that acquires the first state
        if not os.path.isfile(self.speakers_info_file_path):
            self.logger.info("First user!")
            # This function creates the speakers_info and the speakers_sequence_stats files and initializes them
            # with the info of a generic user
            welcome_sentence_str = self.utils.acquire_initial_state()
            # Retrieve starting nuance vectors
            with open(self.nuance_vectors_file_path) as f:
                self.nuance_vectors = json.load(f)
            welcome_str = welcome_sentence_str
            self.logger.info(str(welcome_str))
        else:
            self.logger.info("Users are already present in the info file")
            if language == "it":
                welcome_back_msg = "È bello rivederti! Di cosa vorresti parlare?"
            else:
                welcome_back_msg = "Welcome back! I missed you. What would you like to talk about?"
            welcome_str = welcome_back_msg

        self.previous_sentence = welcome_str
        self.animated_speech.say(self.voice_speed + str(welcome_str), self.configuration)

        # Retrieve the states of the users and save them in a dictionary
        with open(self.dialogue_state_file_path) as f:
            self.dialogue_state = DialogueState(d=json.load(f))

        # If it is the first time, fill the nuance vectors from the file
        if len(self.nuance_vectors) != 0:
            self.dialogue_state.dialogue_nuances = self.nuance_vectors

        # Store the welcome or welcome back string in the assistant field
        self.dialogue_state.conversation_history.append({"role": "assistant", "content": welcome_str})

        # Retrieve the info of the users and store them in a dictionary
        with open(self.speakers_info_file_path) as f:
            self.speakers_info = json.load(f)

        # Retrieve dialogue statistics file
        with open(self.dialogue_statistics_file_path) as f:
            self.dialogue_statistics = DialogueStatistics(d=json.load(f))

        self.dialogue_state.prev_dialogue_sentence = [["s", self.previous_sentence]]
        prev_turn_last_speaker = ""
        prev_speaker_topic = ""

        # If dense captioning should be used, start the thread to update visual information
        if self.dense_cap:
            t1 = threading.Thread(None, self.acquire_image)
            t1.start()

        while self.isAlive:
            self.logger.info("** Listening **")
            # Tell the audio recorder that the client is ready to receive the user reply
            self.client_socket.send(self.dialogue_state.sentence_type.encode("utf-8"))
            # The first string received is an ack that the user has finished talking (after 2s)
            received_str = self.client_socket.recv(1024).decode('utf-8')
            self.check_socket_connection(received_str)
            self.client_socket.send("ack".encode('utf-8'))
            # When using openAI say something to fill the void while waiting
            random_sent_thread = threading.Thread(target=self.say_sentence, args=())
            random_sent_thread.start()

            # The second string received should be the xml client sentence
            xml_string = self.client_socket.recv(1024).decode('utf-8')
            self.check_socket_connection(xml_string)

            # Do not proceed until the xml string is complete and all tags are closed
            proceed = False
            while not proceed:
                try:
                    ET.ElementTree(ET.fromstring(xml_string))
                    proceed = True
                except xml.etree.ElementTree.ParseError:
                    # If the xml is not complete, read again from the socket
                    self.logger.info("The XML is not complete.")
                    xml_string = xml_string + self.client_socket.recv(1024).decode('utf-8')

            # Create a dialogue turn object starting from the xml
            dialogue_turn = DialogueTurn(xml_string)

            # Update the dialogue statistics only if the required minimum number of users is registered
            if len(self.dialogue_statistics.mapping_index_speaker) > 1:
                self.dialogue_statistics.update_statistics(dialogue_turn, prev_turn_last_speaker)

                # Update content of the speaker stats file after having updated them after someone talked
                with open(self.dialogue_statistics_file_path, 'w') as cl_state:
                    json.dump(self.dialogue_statistics.to_dict(), cl_state, ensure_ascii=False, indent=4)

            # Parse the xml string and extract the first sentence and the first speaker
            tree = ET.ElementTree(ET.fromstring(xml_string))
            speaker_id = tree.findall('profile_id')[0]
            sentence = tree.findall('profile_id')[0].text.strip('.,!?')
            self.logger.info(str(sentence))

            # Check if the user wants to exit or wants the robot to repeat the previous sentence
            sentence = sentence.replace(".", "")
            # Reset repeat to false, otherwise it will always repeat the previous sentence
            self.repeat = False

            # If the user said one of the "Exit Application keywords"
            if any(exit_sent in sentence for exit_sent in self.exit_keywords):
                self.isAlive = False
                if language == "it":
                    goodbye_msg = "Ok, è stato bello passare del tempo insieme! A presto!"
                else:
                    goodbye_msg = "Ok, it was a pleasure talking with you! Goodbye."
                self.animated_speech.say(self.voice_speed + goodbye_msg, self.configuration)
                self.memory.insertData(self.speech_reco_event, [])
                self.onInput_onStop()
                sys.exit(0)
            # If the user said a Repeat keyword
            elif sentence.lower() in self.repeat_keywords:
                # If a previous sentence to repeat exists
                if self.previous_sentence:
                    self.repeat = True
                    if language == "it":
                        repeat_msg = "Certamente. Ho detto: "
                    else:
                        repeat_msg = "Sure! I said: "
                    self.animated_speech.say(self.voice_speed + repeat_msg + str(self.previous_sentence[1]),
                                             self.configuration)
                else:
                    if language == "it":
                        repeat_msg = "Mi dispiace, non ho niente da ripetere."
                    else:
                        repeat_msg = "I'm sorry, I have nothing to repeat."
                    self.animated_speech.say(self.voice_speed + repeat_msg, self.configuration)

            # If the user did not ask to exit or to repeat something, send the sentence to the server
            if not self.repeat:
                # Store the user sentence in the conversation history of the dialogue state and pop the first item if needed
                if len(self.dialogue_state.conversation_history) >= max_history_turns:
                    self.dialogue_state.conversation_history.pop(0)
                    print(self.dialogue_state.conversation_history)
                self.dialogue_state.conversation_history.append({"role": "user", "content": sentence})

                # Copy the speakers info in a dictionary that does not contain the names
                # This is needed by OpenAI as it should know the gender.
                speakers_info_no_names = {}
                for speaker_id in self.speakers_info:
                    speakers_info_no_names[speaker_id] = {"gender": self.speakers_info[speaker_id]["gender"],
                                                          "age": self.speakers_info[speaker_id]["age"]}

                # Compose the payload of the message to be sent to the server
                data = {"req_type": 1, "client_sentence": xml_string, "dialogue_state": self.dialogue_state.to_dict(),
                        "dialogue_statistics": self.dialogue_statistics.to_dict(), "speakers_info": speakers_info_no_names,
                        "prev_speaker_info": {"id": prev_turn_last_speaker, "topic": prev_speaker_topic},
                        "dense_cap_result": self.dense_cap_result}

                # Update the info about id and topic of previous speaker to the current one
                prev_turn_last_speaker = dialogue_turn.turn_pieces[-1].profile_id
                prev_speaker_topic = self.dialogue_state.topic

                # Create the thread for the first request
                req_thread = threading.Thread(target=self.hub_request, args=(data,))
                req_thread.start()
                # Wait for the thread of the random sentence
                random_sent_thread.join()
                # Wait for the thread to finish
                req_thread.join()

                # If there is a plan sentence, it means that something has been matched by the Plan manager service
                if self.plan_sentence:
                    self.logger.info(str(self.plan_sentence))
                    self.plan_sentence = self.utils.replace_speaker_name(self.plan_sentence, self.speakers_info)
                    self.plan_sentence = self.utils.replace_schwa_in_string(self.plan_sentence, self.speakers_info, speaker_id)
                    self.animated_speech.say(self.voice_speed + str(self.plan_sentence), self.configuration)

                # If there is a plan, execute it (if the behavior is installed)
                if self.plan:
                    plan_items = self.plan.split("#")[1:]
                    self.logger.info(plan_items)
                    # For each action in the plan, check which action is it and execute it
                    for item in plan_items:
                        action = re.findall("action=(\w+)", item)[0]
                        if action == "registration":
                            # The function that manages the registration, updates the files and returns the updated
                            # dictionaries, so that we don't have to read from the files at each turn.
                            self.speakers_info, self.dialogue_statistics = self.utils.registration_procedure()
                        else:
                            item = item.encode('utf-8')
                            self.plans.perform_action(item)
                self.voice_speed = "\\RSPD=" + str(self.memory.getData("CAIR/voice_speed")) + "\\"

                if self.tablet:
                    self.tablet_service.showImage("http://" + self.tablet_service.robotIp() + "/apps/" + app_name + "/img/RICE.png")
                # Make a copy of the dialogue sentence before it is modified by the second request
                dialogue_sentence1 = self.dialogue_sentence
                dialogue_sentence1_history = self.utils.process_sentence(dialogue_sentence1, self.speakers_info)
                # Store the assistant sentence in the conversation history of the dialogue state
                if len(self.dialogue_state.conversation_history) >= max_history_turns:
                    self.dialogue_state.conversation_history.pop(0)
                    print(self.dialogue_state.conversation_history)
                self.dialogue_state.conversation_history.append(
                    {"role": "assistant", "content": dialogue_sentence1_history})

                dialogue_sentence1_str = self.utils.replace_speaker_name(dialogue_sentence1_history, self.speakers_info)
                self.logger.info(str(dialogue_sentence1_str))

                if openai:
                    # Update the data content for the second request - update also the dialogue state!
                    data["req_type"] = 2
                    data["dialogue_state"] = self.dialogue_state.to_dict()
                    data["dense_cap_result"] = self.dense_cap_result
                    # Create a thread that performs another request to get the continuation of the dialogue
                    req_thread = threading.Thread(target=self.hub_request, args=(data,))
                    req_thread.start()

                dialogue_sentence1_str = dialogue_sentence1_str.replace("robot", "ròbot")
                # Log the time taken to say the first dialogue sentence
                start_time = time.time()
                try:
                    self.animated_speech.say(self.voice_speed + str(dialogue_sentence1_str), self.configuration)
                except:
                    self.animated_speech.say(self.voice_speed + "Mi dispiace, la frase contiene caratteri che non sono in grado di leggere!", self.configuration)
                end_time = time.time()
                to_log = "d#first_sentence_speaking_time:" + str(end_time-start_time) + "\n"
                self.log_socket.send(to_log.encode('utf-8'))

                if openai:
                    if req_thread.is_alive():
                        req_thread.join()
                    dialogue_sentence2 = self.dialogue_sentence[1:]
                    dialogue_sentence2_history = self.utils.process_sentence(dialogue_sentence2, self.speakers_info)

                    dialogue_sentence2_str = self.utils.replace_speaker_name(dialogue_sentence2_history, self.speakers_info)
                    self.logger.info(str(dialogue_sentence2_str))
                    dialogue_sentence2_str = dialogue_sentence2_str.replace("robot", "ròbot")
                    # Log the time taken to say the second dialogue sentence
                    start_time = time.time()
                    try:
                        self.animated_speech.say(self.voice_speed + str(dialogue_sentence2_str), self.configuration)
                    except:
                        self.animated_speech.say(self.voice_speed + "Mi dispiace, la frase contiene caratteri che non sono in grado di leggere!", self.configuration)
                    end_time = time.time()
                    to_log = "d#second_sentence_speaking_time:" + str(end_time-start_time) + "\n"
                    self.log_socket.send(to_log.encode('utf-8'))
                    self.log_socket.send("d#********************\n".encode('utf-8'))
                else:
                    dialogue_sentence2_history = ""

                # Replace the last assistant reply with the complete one
                self.dialogue_state.conversation_history.pop()
                self.dialogue_state.conversation_history.append(
                    {"role": "assistant", "content": dialogue_sentence1_history + " " + dialogue_sentence2_history})

                self.previous_sentence = self.dialogue_sentence
                self.dialogue_state.prev_dialogue_sentence = self.dialogue_sentence

    def onUnload(self):
        pass

    def onInput_onStop(self):
        if self.tablet:
            self.tablet_service.hideImage()
        self.al.setState("interactive")
        self.utils.setAutonomousAbilities(True, True, True, True, True)
        # If present, delete the transformation - if the robot moves outside CAIR it is no more valid
        try:
            self.memory.removeData("CAIR/transformation_matrix")
            self.memory.removeData("CAIR/theta")
        except:
            self.logger.info("No transformation to delete in memory.")
            pass
        self.onUnload()
        self.onStopped()
        sys.exit(0)]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /></Box><Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" /><Link inputowner="0" indexofinput="4" outputowner="1" indexofoutput="4" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box></ChoregrapheProject>