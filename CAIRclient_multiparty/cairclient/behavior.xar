<?xml version="1.0" encoding="UTF-8" ?><ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3"><Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0"><bitmap>media/images/box/root.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram><Box name="CAIRclient multiparty" id="1" localization="8" tooltip="This box contains a basic python script and can be used to create any python script box you would like.&#x0A;&#x0A;To edit its script, double-click on it." x="320" y="123"><bitmap>media/images/box/box-python-script.png</bitmap><script language="4"><content><![CDATA[#!usr/bin/python -tt
# -*- coding: utf-8 -*-
import qi
from naoqi import ALProxy
import sys
reload(sys)
sys.setdefaultencoding("utf-8")
app_name = "cairclient_multiparty"
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/libs/")
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/")
from cairlib.DialogueStatistics import DialogueStatistics
from cairlib.DialogueTurn import DialogueTurn
from cairlib.DialogueState import DialogueState
from cairlib.CAIRclient_SoftBank_utils import Utils
from cairlib.CAIRclient_SoftBank_actions import ActionManager
import xml.etree.ElementTree as ET
import re
import xml
import requests
import os
import json
import socket
import zlib
import threading
import random
import base64
import time

# Location of the server
cineca = "131.175.205.146"
local = "130.251.13.167"
ip = local
server_ip = ip
audio_recorder_ip = ip
registration_ip = ip
openai = True
if openai:
    server_port = "5005"
else:
    server_port = "5000"
request_uri = "http://" + server_ip + ":" + server_port + "/CAIR_hub"

# Dense captioning - if set to false the dense_cap_result will be sent empty to the server and the
# visual information will not be used by gpt-4
dense_cap = True
dense_cap_result = []
img_port = "5100"
img_url = "http://" + server_ip + ":" + img_port + "/CAIR_dense_captioning"
max_history_turns = 6
language = "it"


class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        # Instances of the classes of the other files in the libs folder containing functions needed here
        self.memory = ALProxy("ALMemory")
        self.logger.info("Setting voice speed")
        try:
            self.voice_speed = "\\RSPD=100\\"
            self.memory.insertData("CAIR/voice_speed", 100)
            # self.voice_speed = "\\RSPD=" + str(self.memory.getData("CAIR/voice_speed")) + "\\"
        except:
            self.memory.insertData("CAIR/voice_speed", 100)
            self.voice_speed = "\\RSPD=100\\"

        self.memory.insertData("CAIR/server_ip", server_ip)
        self.memory.insertData("CAIR/server_port", server_port)
        self.memory.insertData("CAIR/registration_ip", registration_ip)
        self.memory.insertData("CAIR/language", language)
        self.memory.insertData("CAIR/app_name", app_name)
        self.plans = ActionManager(self.logger.info)
        self.utils = Utils(self.logger.info)
        self.isAlive = True
        self.exit_keywords = ["stop talking", "esci dallapp", "esci dallapplicazione", "quit the application"]
        self.repeat_keywords = ["repeat", "can you repeat", "say it again", "puoi ripetere", "ripeti", "non ho capito"]
        self.speech_reco_event = "Audio/RecognizedWords"
        self.al = ALProxy("ALAutonomousLife")
        self.motion = ALProxy("ALMotion")
        self.tts = ALProxy("ALTextToSpeech")
        if language == "it":
            self.tts.setLanguage("Italian")
            self.not_installed_behavior = "Mi dispiace, non sono ancora capace di svolgere questa azione perché l'applicazione non è installata."
        else:
            self.tts.setLanguage("English")
            self.not_installed_behavior = "I'm sorry, I am not able to perform this task as the application is not installed."
        self.animated_speech = ALProxy("ALAnimatedSpeech")
        self.configuration = {"bodyLanguageMode": "contextual"}
        self.behavior_manager = ALProxy("ALBehaviorManager")
        self.audio_device = ALProxy("ALAudioDevice")
        self.dialogue_state_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                        "dialogue_state.json"
        self.speakers_info_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                       "speakers_info.json"
        self.dialogue_statistics_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                             "dialogue_statistics.json"
        self.response_times_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                        "response_times.txt"

        self.nuance_vectors_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/" \
                                        "nuance_vectors.json"

        self.logger.info("Trying to connect to ALTabletService...")
        self.tablet = True
        try:
            self.tablet_service = ALProxy("ALTabletService")
        except:
            self.tablet = False

        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        self.audio_device = ALProxy("ALAudioDevice")
        self.audio_device.setOutputVolume(50)

        # To store the previous sentence said by the robot
        self.previous_sentence = ""
        self.dialogue_sentence = []
        self.dialogue_state = {}
        self.dialogue_statistics = {}
        self.speakers_info = {}
        self.nuance_vectors = {}
        self.conversation_history = {}
        self.plan_sentence = ""
        self.plan = ""
        # This variable tells if the user want the robot to repeat a sentence
        self.repeat = False

        # Read the sentences in the correct language from the file
        if language == "it":
            sentences_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/sentences_it.txt"
        else:
            sentences_file_path = "/data/home/nao/.local/share/PackageManager/apps/" + app_name + "/sentences_en.txt"
        self.sentences = []
        with open(sentences_file_path) as f:
            self.sentences = [line.rstrip() for line in f]

        self.dense_cap = dense_cap
        self.dense_cap_result = dense_cap_result
        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        self.photoCaptureProxy = ALProxy("ALPhotoCapture", "127.0.0.1", 9559)
        self.photoCaptureProxy.setResolution(2)
        self.photoCaptureProxy.setPictureFormat("jpg")

    def acquire_image(self):
        while True:
            self.photoCaptureProxy.takePictures(1, "/home/nao/recordings/cameras/", "camera_image", True)
            with open("/home/nao/recordings/cameras/camera_image.jpg", "rb") as img_file:
                img_encoded = base64.b64encode(img_file.read()).decode('utf-8')
            # Create a dictionary with the encoded image
            data = {"frame": img_encoded}
            # Send the image to the server using a POST request
            response = requests.post(img_url, json=data)
            # Print the server's response
            self.dense_cap_result = response.json()["result"]
            time.sleep(3.5)

    def hub_request(self, data):
        encoded_data = json.dumps(data).encode('utf-8')
        compressed_data = zlib.compress(encoded_data)
        hub_response = requests.put(request_uri, data=compressed_data, verify=False)
        # If the Hub cannot contact the dialogue service, the response will be empty
        if hub_response:
            # Overwrite the array containing the states of the profiles with those contained in the Hub response
            # The speakers info are not sent to the Hub.
            self.dialogue_state = DialogueState(d=hub_response.json()['dialogue_state'])
            # Store the updated dialogue state in the file
            with open("dialogue_state.json", 'w') as f:
                json.dump(self.dialogue_state.to_dict(), f, ensure_ascii=False, indent=4)
            self.dialogue_sentence = hub_response.json()['dialogue_sentence']

            if data["req_type"] == 1:
                self.dialogue_statistics = DialogueStatistics(d=hub_response.json()["dialogue_statistics"])
                # The hub updates the average topic distance matrix, hence it should be written on the file
                with open("dialogue_statistics.json", 'w') as f:
                    json.dump(self.dialogue_statistics.to_dict(), f, ensure_ascii=False, indent=4)
                self.plan_sentence = hub_response.json()['plan_sentence']
                self.plan = hub_response.json()['plan']
        else:
            print("No response received from the Hub!")
            exit(0)

    def onInput_onStart(self):
        # Try connecting to the socket that records the audio
        self.logger.info("Trying to connect to the audio recorder socket.")
        try:
            self.client_socket.connect((audio_recorder_ip, 9090))
        except:
            if language == "it":
                to_say = "Mi dispiace, non riesco a connettermi al microfono esterno. Controlla l'indirizzo I P e riprova."
            else:
                to_say = "I'm sorry, I can't connect to external microphone. Check the IP address and try again."
            self.animated_speech.say(self.voice_speed + to_say, self.configuration)
            self.onInput_onStop()
        try:
            if self.al.getState() != "disabled":
                self.al.setState("disabled")
        except:
            pass

        # Wake the robot up if not already up
        self.motion.wakeUp()

        # With Pepper robot, preload the images to be set on the tablet during the conversation
        if self.tablet:
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/DialogueMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/ExecutionMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/" + app_name + "/img/PrivacyMode.png")
            self.tablet_service.showImage("http://" + self.tablet_service.robotIp() +
                                          "/apps/" + app_name + "/img/DialogueMode.png")

        # If it's the first time using the system, call the function that acquires the first state
        if not os.path.isfile(self.speakers_info_file_path):
            self.logger.info("First user!")
            # This function creates the speakers_info and the speakers_sequence_stats files and initializes them
            # with the info of a generic user
            welcome_sentence_str = self.utils.acquire_initial_state()
            # Retrieve starting nuance vectors
            with open(self.nuance_vectors_file_path) as f:
                self.nuance_vectors = json.load(f)
            welcome_str = welcome_sentence_str
            self.logger.info(str(welcome_str))
        else:
            self.logger.info("Users are already present in the info file")
            if language == "it":
                welcome_back_msg = "È bello rivedervi! Di cosa vorreste parlare?"
            else:
                welcome_back_msg = "Welcome back! I missed you. What would you like to talk about?"
            welcome_str = welcome_back_msg

        self.previous_sentence = welcome_str
        self.animated_speech.say(self.voice_speed + str(welcome_sentence_str), self.configuration)

        # Retrieve the states of the users and save them in a dictionary
        with open(self.dialogue_state_file_path) as f:
            self.dialogue_state = DialogueState(d=json.load(f))

        # If it is the first time, fill the nuance vectors from the file
        if len(self.nuance_vectors) != 0:
            self.dialogue_state.dialogue_nuances = self.nuance_vectors
        # Store the welcome or welcome back string in the assistant field
        self.dialogue_state.conversation_history.append({"role": "assistant", "content": welcome_str})

        # Retrieve the info of the users and store them in a dictionary
        with open(self.speakers_info_file_path) as f:
            self.speakers_info = json.load(f)

        # Retrieve dialogue statistics file
        with open(self.dialogue_statistics_file_path) as f:
            self.dialogue_statistics = DialogueStatistics(d=json.load(f))

        self.dialogue_state.prev_dialogue_sentence = [["s", self.previous_sentence]]
        prev_turn_last_speaker = ""
        prev_speaker_topic = ""

        # If dense captioning should be used, start the thread to update visual information
        if self.dense_cap:
            t1 = threading.Thread(None, self.acquire_image)
            t1.start()

        while self.isAlive:
            self.logger.info("** Listening **")
            # Tell the audio recorder that the client is ready to receive the user reply
            self.client_socket.send(self.dialogue_state.sentence_type.encode("utf-8"))
            xml_string = self.client_socket.recv(1024).decode('utf-8')

            if xml_string == "":
                if language == "it":
                    to_say = "Mi dispiace, c'è stato qualche problema con la connessione al microfono esterno."
                else:
                    to_say = "I'm sorry, there was a problem with the connection to the external microphone."
                self.animated_speech.say(self.voice_speed + to_say, self.configuration)
                self.onInput_onStop()

            # Do not proceed until the xml string is complete and all tags are closed
            proceed = False
            while not proceed:
                try:
                    ET.ElementTree(ET.fromstring(xml_string))
                    proceed = True
                except xml.etree.ElementTree.ParseError:
                    # If the xml is not complete, read again from the socket
                    self.logger.info("The XML is not complete.")
                    xml_string = xml_string + self.client_socket.recv(1024).decode('utf-8')

            # Create a dialogue turn object starting from the xml
            dialogue_turn = DialogueTurn(xml_string)

            # Update the dialogue statistics only if the required minimum number of users is registered
            if len(self.dialogue_statistics.mapping_index_speaker) > 1:
                self.dialogue_statistics.update_statistics(dialogue_turn, prev_turn_last_speaker)

                # Update content of the speaker stats file after having updated them after someone talked
                with open(self.dialogue_statistics_file_path, 'w') as cl_state:
                    json.dump(self.dialogue_statistics.to_dict(), cl_state, ensure_ascii=False, indent=4)

            # Parse the xml string and extract the first sentence and the first speaker
            tree = ET.ElementTree(ET.fromstring(xml_string))
            speaker_id = tree.findall('profile_id')[0]
            sentence = tree.findall('profile_id')[0].text.strip('.,!?')
            self.logger.info(str(sentence))

            # Check if the user wants to exit or wants the robot to repeat the previous sentence
            sentence = sentence.replace(".", "")
            # Reset repeat to false, otherwise it will always repeat the previous sentence
            self.repeat = False

            # If the user said one of the "Exit Application keywords"
            if any(exit_sent in sentence for exit_sent in self.exit_keywords):
                self.isAlive = False
                if language == "it":
                    goodbye_msg = "Ok, è stato bello passare del tempo insieme! A presto!"
                else:
                    goodbye_msg = "Ok, it was a pleasure talking with you! Goodbye."
                self.animated_speech.say(self.voice_speed + goodbye_msg, self.configuration)
                self.memory.insertData(self.speech_reco_event, [])
                self.onInput_onStop()
                sys.exit(0)
            # If the user said a Repeat keyword
            elif sentence.lower() in self.repeat_keywords:
                # If a previous sentence to repeat exists
                if self.previous_sentence:
                    self.repeat = True
                    if language == "it":
                        repeat_msg = "Certamente. Ho detto: "
                    else:
                        repeat_msg = "Sure! I said: "
                    self.animated_speech.say(self.voice_speed + repeat_msg + str(self.previous_sentence[1]),
                                             self.configuration)
                else:
                    if language == "it":
                        repeat_msg = "Mi dispiace, non ho niente da ripetere."
                    else:
                        repeat_msg = "I'm sorry, I have nothing to repeat."
                    self.animated_speech.say(self.voice_speed + repeat_msg, self.configuration)

            # If the user did not ask to exit or to repeat something, send the sentence to the server
            if not self.repeat:
                # Store the user sentence in the conversation history of the dialogue state and pop the first item if needed
                if len(self.dialogue_state.conversation_history) >= max_history_turns:
                    self.dialogue_state.conversation_history.pop(0)
                    print(self.dialogue_state.conversation_history)
                self.dialogue_state.conversation_history.append({"role": "user", "content": sentence})

                # Copy the speakers info in a dictionary that does not contain the names
                # This is needed by OpenAI as it should know the gender.
                speakers_info_no_names = {}
                for speaker_id in self.speakers_info:
                    speakers_info_no_names[speaker_id] = {"gender": self.speakers_info[speaker_id]["gender"],
                                                          "age": self.speakers_info[speaker_id]["age"]}

                # Compose the payload of the message to be sent to the server
                data = {"req_type": 1, "client_sentence": xml_string, "dialogue_state": self.dialogue_state.to_dict(),
                        "dialogue_statistics": self.dialogue_statistics.to_dict(), "speakers_info": speakers_info_no_names,
                        "prev_speaker_info": {"id": prev_turn_last_speaker, "topic": prev_speaker_topic},
                        "dense_cap_result": self.dense_cap_result}

                # Update the info about id and topic of previous speaker to the current one
                prev_turn_last_speaker = dialogue_turn.turn_pieces[-1].profile_id
                prev_speaker_topic = self.dialogue_state.topic

                # Create the thread for the first request
                req_thread = threading.Thread(target=self.hub_request, args=(data,))
                req_thread.start()

                # When using openAI say something to fill the void while waiting
                if openai:
                    to_say = str(random.choice(self.sentences))
                    self.animated_speech.say(self.voice_speed + to_say, self.configuration)

                # Wait for the thread to finish
                req_thread.join()

                # If there is a plan sentence, it means that something has been matched by the Plan manager service
                if self.plan_sentence:
                    self.logger.info(str(self.plan_sentence))
                    self.utils.replace_speaker_name(self.plan_sentence, self.speakers_info)
                    self.plan_sentence = self.utils.replace_schwa_in_string(self.plan_sentence, self.speakers_info, speaker_id)
                    self.animated_speech.say(self.voice_speed + str(self.plan_sentence), self.configuration)

                # If there is a plan, execute it (if the behavior is installed)
                if self.plan:
                    plan_items = self.plan.split("#")[1:]
                    self.logger.info(plan_items)
                    # For each action in the plan, check which action is it and execute it
                    for item in plan_items:
                        action = re.findall("action=(\w+)", item)[0]
                        if action == "registration":
                            # The function that manages the registration, updates the files and returns the updated
                            # dictionaries, so that we don't have to read from the files at each turn.
                            self.speakers_info, self.dialogue_statistics = self.utils.registration_procedure()
                        else:
                            item = item.encode('utf-8')
                            self.plans.perform_action(item)

                # Make a copy of the dialogue sentence before it is modified by the second request
                dialogue_sentence1 = self.dialogue_sentence
                dialogue_sentence1_history = self.utils.process_sentence(dialogue_sentence1, self.speakers_info)
                # Store the assistant sentence in the conversation history of the dialogue state
                if len(self.dialogue_state.conversation_history) >= max_history_turns:
                    self.dialogue_state.conversation_history.pop(0)
                    print(self.dialogue_state.conversation_history)
                self.dialogue_state.conversation_history.append(
                    {"role": "assistant", "content": dialogue_sentence1_history})

                dialogue_sentence1_str = self.utils.replace_speaker_name(dialogue_sentence1_history, self.speakers_info)
                if openai:
                    # Update the data content for the second request - update also the dialogue state!
                    data["req_type"] = 2
                    data["dialogue_state"] = self.dialogue_state.to_dict()
                    data["dense_cap_result"] = self.dense_cap_result
                    # Create a thread that performs another request to get the continuation of the dialogue
                    req_thread = threading.Thread(target=self.hub_request, args=(data,))
                    req_thread.start()

                self.animated_speech.say(self.voice_speed + str(dialogue_sentence1_str), self.configuration)

                if req_thread.is_alive():
                    req_thread.join()
                dialogue_sentence2 = self.dialogue_sentence[1:]
                dialogue_sentence2_history = self.utils.process_sentence(dialogue_sentence2, self.speakers_info)

                dialogue_sentence2_str = self.utils.replace_speaker_name(dialogue_sentence2_history, self.speakers_info)
                self.animated_speech.say(self.voice_speed + str(dialogue_sentence2_str), self.configuration)

                # Replace the last assistant reply with the complete one
                self.dialogue_state.conversation_history.pop()
                self.dialogue_state.conversation_history.append(
                    {"role": "assistant", "content": dialogue_sentence1_history + " " + dialogue_sentence2_history})

                self.previous_sentence = self.dialogue_sentence
                self.dialogue_state.prev_dialogue_sentence = self.dialogue_sentence

    def onUnload(self):
        pass

    def onInput_onStop(self):
        if self.tablet:
            self.tablet_service.hideImage()
        self.al.setState("interactive")
        self.utils.setAutonomousAbilities(True, True, True, True, True)
        # If present, delete the transformation - if the robot moves outside CAIR it is no more valid
        try:
            self.memory.removeData("CAIR/transformation_matrix")
            self.memory.removeData("CAIR/theta")
        except:
            self.logger.info("No transformation to delete in memory.")
            pass
        self.onUnload()
        self.onStopped()
        sys.exit(0)]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /></Box><Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" /><Link inputowner="0" indexofinput="4" outputowner="1" indexofoutput="4" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box></ChoregrapheProject>